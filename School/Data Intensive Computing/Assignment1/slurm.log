    T_INFO : Hello! My 2-node cluster is running on the following nodes :
    T_INFO : And here are the associated IP addressees for each node:
    node-061
    node-062
    WARNING: Stopping all Apache Hadoop daemons as twagenhals in 10 seconds.
    WARNING: Use CTRL-C to abort.
    Stopping namenodes on [node-061]
    Stopping datanodes
    Stopping secondary namenodes [node-061]
    Stopping nodemanagers
    Stopping resourcemanager
    T_INFO : executing 'rm -rf /tmp/hadoop-twagenhals on node-061'
    T_INFO : executing 'mkdir /tmp/hadoop-twagenhals on node-061'
    mkdir: cannot create directory '/tmp/hadoop-twagenhals': File exists
    T_INFO : created directory /tmp/hadoop-twagenhals/
    T_INFO : executing 'rm -rf /tmp/hadoop-twagenhals on node-062'
    T_INFO : executing 'mkdir /tmp/hadoop-twagenhals on node-062'
    mkdir: cannot create directory '/tmp/hadoop-twagenhals': File exists
    T_INFO : created directory /tmp/hadoop-twagenhals/
    2019-02-23 19:51:35,429 INFO namenode.NameNode: STARTUP_MSG:
    /************************************************************
    STARTUP_MSG: Starting NameNode
    STARTUP_MSG:   host = node-061/10.2.0.55
    STARTUP_MSG:   args = [-format, -force]
    STARTUP_MSG:   version = 3.2.0
    STARTUP_MSG:   classpath = /home/gridsan/twagenhals/hadoop/etc/hadoop:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/netty-3.10.5.Final.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jackson-core-2.9.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/commons-codec-1.11.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/json-smart-2.3.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/commons-io-2.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/hadoop-annotations-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jersey-json-1.19.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/hadoop-auth-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jackson-databind-2.9.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jersey-core-1.19.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/zookeeper-3.4.13.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/curator-client-2.12.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/commons-text-1.4.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jersey-server-1.19.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/asm-5.0.4.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/accessors-smart-1.2.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/commons-lang3-3.7.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/curator-framework-2.12.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jackson-annotations-2.9.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/lib/commons-net-3.6.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/hadoop-kms-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/hadoop-common-3.2.0-tests.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/hadoop-common-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/common/hadoop-nfs-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jackson-core-2.9.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/json-smart-2.3.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/commons-io-2.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/xz-1.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.9.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/commons-text-1.4.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/asm-5.0.4.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jettison-1.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/gson-2.2.4.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.9.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/lib/commons-net-3.6.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.0-tests.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.0-tests.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.0-tests.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.0-tests.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.0-tests.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/objenesis-1.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.5.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-submarine-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.2.0.jar:/home/gridsan/twagenhals/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.2.0.jar
    STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-08T06:08Z
    STARTUP_MSG:   java = 1.8.0_171
    ************************************************************/
    2019-02-23 19:51:35,441 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
    2019-02-23 19:51:35,526 INFO namenode.NameNode: createNameNode [-format, -force]
    Formatting using clusterid: CID-7f3864d5-22f0-421c-a2fc-a49640cf3ec6
    2019-02-23 19:51:36,093 INFO namenode.FSEditLog: Edit logging is async:true
    2019-02-23 19:51:36,110 INFO namenode.FSNamesystem: KeyProvider: null
    2019-02-23 19:51:36,111 INFO namenode.FSNamesystem: fsLock is fair: true
    2019-02-23 19:51:36,113 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
    2019-02-23 19:51:36,119 INFO namenode.FSNamesystem: fsOwner             = twagenhals (auth:SIMPLE)
    2019-02-23 19:51:36,119 INFO namenode.FSNamesystem: supergroup          = supergroup
    2019-02-23 19:51:36,119 INFO namenode.FSNamesystem: isPermissionEnabled = true
    2019-02-23 19:51:36,119 INFO namenode.FSNamesystem: HA Enabled: false
    2019-02-23 19:51:36,172 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
    2019-02-23 19:51:36,189 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
    2019-02-23 19:51:36,189 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
    2019-02-23 19:51:36,194 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
    2019-02-23 19:51:36,195 INFO blockmanagement.BlockManager: The block deletion will start around 2019 Feb 23 19:51:36
    2019-02-23 19:51:36,196 INFO util.GSet: Computing capacity for map BlocksMap
    2019-02-23 19:51:36,197 INFO util.GSet: VM type       = 64-bit
    2019-02-23 19:51:36,198 INFO util.GSet: 2.0% max memory 13.9 GB = 285.5 MB
    2019-02-23 19:51:36,198 INFO util.GSet: capacity      = 2^25 = 33554432 entries
    2019-02-23 19:51:36,262 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
    2019-02-23 19:51:36,263 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
    2019-02-23 19:51:36,270 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
    2019-02-23 19:51:36,271 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
    2019-02-23 19:51:36,271 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
    2019-02-23 19:51:36,271 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
    2019-02-23 19:51:36,271 INFO blockmanagement.BlockManager: defaultReplication         = 3
    2019-02-23 19:51:36,271 INFO blockmanagement.BlockManager: maxReplication             = 512
    2019-02-23 19:51:36,271 INFO blockmanagement.BlockManager: minReplication             = 1
    2019-02-23 19:51:36,271 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
    2019-02-23 19:51:36,271 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
    2019-02-23 19:51:36,271 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
    2019-02-23 19:51:36,271 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
    2019-02-23 19:51:36,304 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
    2019-02-23 19:51:36,304 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
    2019-02-23 19:51:36,304 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
    2019-02-23 19:51:36,304 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
    2019-02-23 19:51:36,316 INFO util.GSet: Computing capacity for map INodeMap
    2019-02-23 19:51:36,316 INFO util.GSet: VM type       = 64-bit
    2019-02-23 19:51:36,317 INFO util.GSet: 1.0% max memory 13.9 GB = 142.8 MB
    2019-02-23 19:51:36,317 INFO util.GSet: capacity      = 2^24 = 16777216 entries
    2019-02-23 19:51:36,840 INFO namenode.FSDirectory: ACLs enabled? false
    2019-02-23 19:51:36,840 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
    2019-02-23 19:51:36,840 INFO namenode.FSDirectory: XAttrs enabled? true
    2019-02-23 19:51:36,840 INFO namenode.NameNode: Caching file names occurring more than 10 times
    2019-02-23 19:51:36,846 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
    2019-02-23 19:51:36,848 INFO snapshot.SnapshotManager: SkipList is disabled
    2019-02-23 19:51:36,853 INFO util.GSet: Computing capacity for map cachedBlocks
    2019-02-23 19:51:36,853 INFO util.GSet: VM type       = 64-bit
    2019-02-23 19:51:36,853 INFO util.GSet: 0.25% max memory 13.9 GB = 35.7 MB
    2019-02-23 19:51:36,853 INFO util.GSet: capacity      = 2^22 = 4194304 entries
    2019-02-23 19:51:36,864 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
    2019-02-23 19:51:36,864 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
    2019-02-23 19:51:36,864 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
    2019-02-23 19:51:36,868 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
    2019-02-23 19:51:36,868 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
    2019-02-23 19:51:36,870 INFO util.GSet: Computing capacity for map NameNodeRetryCache
    2019-02-23 19:51:36,870 INFO util.GSet: VM type       = 64-bit
    2019-02-23 19:51:36,870 INFO util.GSet: 0.029999999329447746% max memory 13.9 GB = 4.3 MB
    2019-02-23 19:51:36,870 INFO util.GSet: capacity      = 2^19 = 524288 entries
    Data exists in Storage Directory root= /tmp/hadoop-twagenhals/dfs/name; location= null. Formatting anyway.
    2019-02-23 19:51:36,901 INFO namenode.FSImage: Allocated new BlockPoolId: BP-351935541-10.2.0.55-1550969496893
    2019-02-23 19:51:36,901 INFO common.Storage: Will remove files: [/tmp/hadoop-twagenhals/dfs/name/current/edits_inprogress_0000000000000000001, /tmp/hadoop-twagenhals/dfs/name/current/fsimage_0000000000000000000.md5, /tmp/hadoop-twagenhals/dfs/name/current/fsimage_0000000000000000000, /tmp/hadoop-twagenhals/dfs/name/current/VERSION, /tmp/hadoop-twagenhals/dfs/name/current/seen_txid]
    2019-02-23 19:51:37,015 INFO common.Storage: Storage directory /tmp/hadoop-twagenhals/dfs/name has been successfully formatted.
    2019-02-23 19:51:37,032 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-twagenhals/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
    2019-02-23 19:51:37,247 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-twagenhals/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 405 bytes saved in 0 seconds .
    2019-02-23 19:51:37,279 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
    2019-02-23 19:51:37,288 INFO namenode.NameNode: SHUTDOWN_MSG:
    /************************************************************
    SHUTDOWN_MSG: Shutting down NameNode at node-061/10.2.0.55
    ************************************************************/
    Starting namenodes on [node-061]
    Starting datanodes
    Starting secondary namenodes [node-061]
    Starting resourcemanager
    Starting nodemanagers
    Configured Capacity: 0 (0 B)
    Present Capacity: 0 (0 B)
    DFS Remaining: 0 (0 B)
    DFS Used: 0 (0 B)
    DFS Used%: 0.00%
    Replicated Blocks:
            Under replicated blocks: 0
            Blocks with corrupt replicas: 0
            Missing blocks: 0
            Missing blocks (with replication factor 1): 0
            Low redundancy blocks with highest priority to recover: 0
            Pending deletion blocks: 0
    Erasure Coded Block Groups:
            Low redundancy block groups: 0
            Block groups with corrupt internal blocks: 0
            Missing block groups: 0
            Low redundancy blocks with highest priority to recover: 0
            Pending deletion blocks: 0

    -------------------------------------------------
    Found 1 items
    drwxr-xr-x   - twagenhals supergroup          0 2019-02-23 19:52 /user/twagenhals
    T_INFO : User directories created.
    2019-02-23 19:52:07,668 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/hdfs-site.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/hdfs-site.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:07,766 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/master._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/master._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:07,794 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/yarnservice-log4j.properties._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/yarnservice-log4j.properties._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:07,820 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/#test#._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/#test#._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:07,845 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/hadoop-env.cmd._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/hadoop-env.cmd._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:07,870 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/core-site.xml~._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/core-site.xml~._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:07,892 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/yarn-site.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/yarn-site.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:07,915 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/kms-log4j.properties._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/kms-log4j.properties._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:07,937 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/yarn-env.cmd._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/yarn-env.cmd._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:07,959 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/ssl-client.xml.example._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/ssl-client.xml.example._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:07,986 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/httpfs-site.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/httpfs-site.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,009 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/hadoop-metrics2.properties._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/hadoop-metrics2.properties._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,032 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/workers._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/workers._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,078 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/configuration.xsl._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/configuration.xsl._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,102 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/hadoop-policy.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/hadoop-policy.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,125 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/masters._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/masters._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,147 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/core-site2.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/core-site2.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,191 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/#hadoop-env.sh#._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/#hadoop-env.sh#._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,210 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/capacity-scheduler.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/capacity-scheduler.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,230 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/hadoop-env.sh._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/hadoop-env.sh._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,261 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/shellprofile.d/example.sh._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/shellprofile.d/example.sh._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,292 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/log4j.properties._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/log4j.properties._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,314 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/ssl-server.xml.example._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/ssl-server.xml.example._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,338 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/container-executor.cfg._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/container-executor.cfg._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,361 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/user_ec_policies.xml.template._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/user_ec_policies.xml.template._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,385 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/#core-site.xml#._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/#core-site.xml#._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,405 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/hadoop-env.sh~._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/hadoop-env.sh~._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,446 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/kms-acls.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/kms-acls.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,468 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/httpfs-signature.secret._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/httpfs-signature.secret._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,490 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/kms-env.sh._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/kms-env.sh._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,509 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/kms-site.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/kms-site.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,528 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/httpfs-env.sh._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/httpfs-env.sh._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,575 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/hadoop-user-functions.sh.example._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/hadoop-user-functions.sh.example._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,597 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/mapred-env.cmd._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/mapred-env.cmd._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,615 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/slaves._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/slaves._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,633 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/core-site.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/core-site.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,652 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/mapred-env.sh._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/mapred-env.sh._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,683 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/mapred-queues.xml.template._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/mapred-queues.xml.template._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,702 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/mapred-site.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/mapred-site.xml._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,720 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/hdfs-site.xml~._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/hdfs-site.xml~._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,740 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/yarn-env.sh._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/yarn-env.sh._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    2019-02-23 19:52:08,760 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/twagenhals/input/hadoop/httpfs-log4j.properties._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    put: File /user/twagenhals/input/hadoop/httpfs-log4j.properties._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
    T_INFO : Hadoop config directory copies to hdfs
    2019-02-23 19:52:11,141 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
    2019-02-23 19:52:11,862 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/twagenhals/.staging/job_1550969513953_0001
    2019-02-23 19:52:11,981 WARN hdfs.DataStreamer: DataStreamer Exception
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /tmp/hadoop-yarn/staging/twagenhals/.staging/job_1550969513953_0001/job.jar could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    2019-02-23 19:52:11,987 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/twagenhals/.staging/job_1550969513953_0001
    org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /tmp/hadoop-yarn/staging/twagenhals/.staging/job_1550969513953_0001/job.jar could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
            at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2135)
            at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
            at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2771)
            at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:876)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:567)
            at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
            at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
            at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
            at java.security.AccessController.doPrivileged(Native Method)
            at javax.security.auth.Subject.doAs(Subject.java:422)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
            at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

            at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
            at org.apache.hadoop.ipc.Client.call(Client.java:1457)
            at org.apache.hadoop.ipc.Client.call(Client.java:1367)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
            at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
            at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:513)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
            at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
            at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
            at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
            at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
            at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
            at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
            at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    T_INFO : Mapreduce example executed
    get: `output': No such file or directory
    T_INFO : Output copied from hdfs node to local machine
    WARNING: Stopping all Apache Hadoop daemons as twagenhals in 10 seconds.
    WARNING: Use CTRL-C to abort.
    Stopping namenodes on [node-061]
    Stopping datanodes
    Stopping secondary namenodes [node-061]
    Stopping nodemanagers
    Stopping resourcemanager